{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Union\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from torchvision import transforms,datasets\n",
    "from torchvision.transforms import RandomPerspective,RandomRotation,RandomCrop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1\n",
    "# For reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Net7,Net8_1, Net9_3\n",
    "from utils import calculate_mnist_mean_std,device,GetCorrectPredCount,plot_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device=='cpu':\n",
    "  kwargs = {'batch_size': 64, 'shuffle': True}\n",
    "else:\n",
    "  kwargs = {'batch_size': 128, 'shuffle': True, 'num_workers': 2, 'pin_memory': True}\n",
    "\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_data = datasets.MNIST(root='../../data/', download=True, transform=transforms.ToTensor() ,)\n",
    "print(mnist_data)\n",
    "\n",
    "mean, std = calculate_mnist_mean_std(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        pred = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred, target)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        correct += GetCorrectPredCount(pred, target)\n",
    "        processed += len(data)\n",
    "\n",
    "        pbar.set_description(desc= f'Train: Loss={loss.item():0.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "\n",
    "        train_acc.append(100*correct/processed)\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "    \n",
    "    current_train_accuracy = 100*correct/processed\n",
    "    current_train_loss     = train_loss/len(train_loader)\n",
    "    return(current_train_accuracy, current_train_loss)\n",
    "\n",
    "        \n",
    "        \n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target, reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "            correct += GetCorrectPredCount(output, target)\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    current_test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    current_test_losses   = test_loss\n",
    "    return (current_test_accuracy, current_test_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean=(mean,), std=(std,) )\n",
    "\n",
    "])\n",
    "\n",
    "spl_train_transforms = transforms.Compose([\n",
    "    RandomRotation((-7.0, 7.0), fill=(1,)),\n",
    "    RandomPerspective(distortion_scale=0.5, p=0.24),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean=(mean,), std=(std,) )\n",
    "\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean=(mean,), std=(std,) )\n",
    "])\n",
    "\n",
    "# Train DataSet, DataLoader\n",
    "train_dataset  = datasets.MNIST(root='../../data/',train=True,transform=train_transforms,download=True)\n",
    "train_loader   = torch.utils.data.DataLoader(train_dataset, **kwargs)\n",
    "\n",
    "\n",
    "# Test DataSet, DataLoader\n",
    "test_dataset  = datasets.MNIST(root='../../data/',train=False,transform=test_transforms,download=True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "# APPLIED TRANSFORMS IN DATASET - SPECIAL\n",
    "strain_dataset = datasets.MNIST(root='../../data/',train=True,transform=spl_train_transforms,download=True)\n",
    "strain_loader   = torch.utils.data.DataLoader(strain_dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Net7().to(device)\n",
    "for name,weights in model1.named_parameters():\n",
    "    print(f\"{name}\\t\\t {weights.shape}\")\n",
    "\n",
    "\n",
    "summary(model1,(1,28,28));\n",
    "\n",
    "train_losses = [] ; test_losses = []; train_acc = []; test_acc = []\n",
    "\n",
    "\n",
    "# SAME MODEL BUT DIFFERNT LR\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.3, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1, verbose=True)\n",
    "criterion = F.nll_loss\n",
    "num_epochs = 15\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    train_accuracy, train_loss = train(model1, device, train_loader, optimizer, criterion)\n",
    "    test_accuracy,test_loss    = test(model1, device, test_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "plot_loss_accuracy(train_losses, test_losses, train_acc, test_acc)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Results:`\n",
    "```log\n",
    "\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "├─Sequential: 1-1                        [-1, 10, 24, 24]          --\n",
    "|    └─Conv2d: 2-1                       [-1, 8, 26, 26]           72\n",
    "|    └─BatchNorm2d: 2-2                  [-1, 8, 26, 26]           16\n",
    "|    └─ReLU: 2-3                         [-1, 8, 26, 26]           --\n",
    "|    └─Conv2d: 2-4                       [-1, 10, 24, 24]          720\n",
    "|    └─BatchNorm2d: 2-5                  [-1, 10, 24, 24]          20\n",
    "|    └─ReLU: 2-6                         [-1, 10, 24, 24]          --\n",
    "├─Sequential: 1-2                        [-1, 10, 12, 12]          --\n",
    "|    └─Conv2d: 2-7                       [-1, 10, 24, 24]          100\n",
    "|    └─MaxPool2d: 2-8                    [-1, 10, 12, 12]          --\n",
    "├─Sequential: 1-3                        [-1, 16, 4, 4]            --\n",
    "|    └─Conv2d: 2-9                       [-1, 12, 10, 10]          1,080\n",
    "|    └─BatchNorm2d: 2-10                 [-1, 12, 10, 10]          24\n",
    "|    └─ReLU: 2-11                        [-1, 12, 10, 10]          --\n",
    "|    └─Conv2d: 2-12                      [-1, 14, 8, 8]            1,512\n",
    "|    └─BatchNorm2d: 2-13                 [-1, 14, 8, 8]            28\n",
    "|    └─ReLU: 2-14                        [-1, 14, 8, 8]            --\n",
    "|    └─Conv2d: 2-15                      [-1, 16, 6, 6]            2,016\n",
    "|    └─BatchNorm2d: 2-16                 [-1, 16, 6, 6]            32\n",
    "|    └─ReLU: 2-17                        [-1, 16, 6, 6]            --\n",
    "|    └─Conv2d: 2-18                      [-1, 16, 4, 4]            2,304\n",
    "|    └─BatchNorm2d: 2-19                 [-1, 16, 4, 4]            32\n",
    "|    └─ReLU: 2-20                        [-1, 16, 4, 4]            --\n",
    "├─Sequential: 1-4                        [-1, 10, 4, 4]            --\n",
    "|    └─Conv2d: 2-21                      [-1, 10, 4, 4]            160\n",
    "├─AdaptiveAvgPool2d: 1-5                 [-1, 10, 1, 1]            --\n",
    "==========================================================================================\n",
    "Total params: 8,116\n",
    "Trainable params: 8,116\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 0.85\n",
    "==========================================================================================\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.26\n",
    "Params size (MB): 0.03\n",
    "Estimated Total Size (MB): 0.29\n",
    "==========================================================================================\n",
    "\n",
    "==========================================================================================\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 1\n",
    "Train: Loss=0.0543 Batch_id=468 Accuracy=94.83: 100%|██████████| 469/469 [00:20<00:00, 22.98it/s]\n",
    "Test set: Average loss: 0.0499, Accuracy: 9837/10000 (98.37%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 2\n",
    "Train: Loss=0.0254 Batch_id=468 Accuracy=98.43: 100%|██████████| 469/469 [00:17<00:00, 26.64it/s]\n",
    "Test set: Average loss: 0.0313, Accuracy: 9903/10000 (99.03%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 3\n",
    "Train: Loss=0.0900 Batch_id=468 Accuracy=98.68: 100%|██████████| 469/469 [00:18<00:00, 25.63it/s]\n",
    "Test set: Average loss: 0.0403, Accuracy: 9878/10000 (98.78%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 4\n",
    "Train: Loss=0.0462 Batch_id=468 Accuracy=98.83: 100%|██████████| 469/469 [00:17<00:00, 26.20it/s]\n",
    "Test set: Average loss: 0.0359, Accuracy: 9891/10000 (98.91%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 5\n",
    "Train: Loss=0.0229 Batch_id=468 Accuracy=99.38: 100%|██████████| 469/469 [00:17<00:00, 26.64it/s]\n",
    "Test set: Average loss: 0.0205, Accuracy: 9936/10000 (99.36%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 6\n",
    "Train: Loss=0.0120 Batch_id=468 Accuracy=99.45: 100%|██████████| 469/469 [00:18<00:00, 25.55it/s]\n",
    "Test set: Average loss: 0.0195, Accuracy: 9941/10000 (99.41%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 7\n",
    "Train: Loss=0.0030 Batch_id=468 Accuracy=99.48: 100%|██████████| 469/469 [00:18<00:00, 25.64it/s]\n",
    "Test set: Average loss: 0.0193, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 8\n",
    "Train: Loss=0.0028 Batch_id=468 Accuracy=99.53: 100%|██████████| 469/469 [00:19<00:00, 24.63it/s]\n",
    "Test set: Average loss: 0.0189, Accuracy: 9938/10000 (99.38%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 9\n",
    "Train: Loss=0.0017 Batch_id=468 Accuracy=99.55: 100%|██████████| 469/469 [00:19<00:00, 24.48it/s]\n",
    "Test set: Average loss: 0.0189, Accuracy: 9940/10000 (99.40%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 10\n",
    "Train: Loss=0.0141 Batch_id=468 Accuracy=99.56: 100%|██████████| 469/469 [00:18<00:00, 25.97it/s]\n",
    "Test set: Average loss: 0.0189, Accuracy: 9941/10000 (99.41%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 11\n",
    "Train: Loss=0.0014 Batch_id=468 Accuracy=99.56: 100%|██████████| 469/469 [00:19<00:00, 24.41it/s]\n",
    "Test set: Average loss: 0.0186, Accuracy: 9944/10000 (99.44%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 12\n",
    "Train: Loss=0.0659 Batch_id=468 Accuracy=99.57: 100%|██████████| 469/469 [00:18<00:00, 25.91it/s]\n",
    "Test set: Average loss: 0.0188, Accuracy: 9944/10000 (99.44%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 13\n",
    "Train: Loss=0.0039 Batch_id=468 Accuracy=99.57: 100%|██████████| 469/469 [00:20<00:00, 22.69it/s]\n",
    "Test set: Average loss: 0.0188, Accuracy: 9946/10000 (99.46%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 14\n",
    "Train: Loss=0.0656 Batch_id=468 Accuracy=99.58: 100%|██████████| 469/469 [00:18<00:00, 24.84it/s]\n",
    "Test set: Average loss: 0.0188, Accuracy: 9944/10000 (99.44%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 15\n",
    "Train: Loss=0.0030 Batch_id=468 Accuracy=99.58: 100%|██████████| 469/469 [00:17<00:00, 26.28it/s]\n",
    "Test set: Average loss: 0.0186, Accuracy: 9946/10000 (99.46%)\n",
    "\n",
    "\n",
    "```\n",
    "**Best Training Accuracy:  99.55**\n",
    "\n",
    "**Best Testing Accuracy :  99.43**\n",
    "\n",
    "\n",
    "`Analysis:`\n",
    "1. Model is Over params now after adding batchnorm. \n",
    "2. We have used no data augmentation\n",
    "3. We have used no Dropout layers\n",
    "4. However we reached Test accuracy of 99.43\n",
    "5. We have used GAP layer (the penultimate layer).\n",
    "6. We have also used StepLR for the Learning rate scheduler\n",
    "7. After Adding Batch Norm to every layer model significantly doing better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![net7](./pics/net7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Net8_1().to(device)\n",
    "for name,weights in model2.named_parameters():\n",
    "    print(f\"{name}\\t\\t {weights.shape}\")\n",
    "\n",
    "\n",
    "summary(model2,(1,28,28));\n",
    "\n",
    "train_losses = [] ; test_losses = []; train_acc = []; test_acc = []\n",
    "\n",
    "\n",
    "# SAME MODEL BUT DIFFERNT LR\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.3, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1, verbose=True)\n",
    "criterion = F.nll_loss\n",
    "num_epochs = 15\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    train_accuracy, train_loss = train(model2, device, train_loader, optimizer, criterion)\n",
    "    test_accuracy,test_loss    = test(model2, device, test_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "plot_loss_accuracy(train_losses, test_losses, train_acc, test_acc)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```log\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "├─Sequential: 1-1                        [-1, 10, 24, 24]          --\n",
    "|    └─Conv2d: 2-1                       [-1, 8, 26, 26]           72\n",
    "|    └─BatchNorm2d: 2-2                  [-1, 8, 26, 26]           16\n",
    "|    └─ReLU: 2-3                         [-1, 8, 26, 26]           --\n",
    "|    └─Dropout2d: 2-4                    [-1, 8, 26, 26]           --\n",
    "|    └─Conv2d: 2-5                       [-1, 10, 24, 24]          720\n",
    "|    └─BatchNorm2d: 2-6                  [-1, 10, 24, 24]          20\n",
    "|    └─ReLU: 2-7                         [-1, 10, 24, 24]          --\n",
    "|    └─Dropout2d: 2-8                    [-1, 10, 24, 24]          --\n",
    "├─Sequential: 1-2                        [-1, 10, 12, 12]          --\n",
    "|    └─Conv2d: 2-9   ==========================================================================================\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 1\n",
    "Train: Loss=0.1027 Batch_id=468 Accuracy=93.77: 100%|██████████| 469/469 [00:22<00:00, 20.46it/s]\n",
    "Test set: Average loss: 0.0453, Accuracy: 9865/10000 (98.65%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 2\n",
    "Train: Loss=0.0216 Batch_id=468 Accuracy=97.86: 100%|██████████| 469/469 [00:20<00:00, 22.48it/s]\n",
    "Test set: Average loss: 0.0371, Accuracy: 9880/10000 (98.80%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 3\n",
    "Train: Loss=0.0561 Batch_id=468 Accuracy=98.28: 100%|██████████| 469/469 [00:18<00:00, 25.23it/s]\n",
    "Test set: Average loss: 0.0275, Accuracy: 9910/10000 (99.10%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 4\n",
    "Train: Loss=0.0140 Batch_id=468 Accuracy=98.53: 100%|██████████| 469/469 [00:18<00:00, 24.76it/s]\n",
    "Test set: Average loss: 0.0269, Accuracy: 9912/10000 (99.12%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 5\n",
    "Train: Loss=0.0063 Batch_id=468 Accuracy=99.02: 100%|██████████| 469/469 [00:19<00:00, 23.81it/s]\n",
    "Test set: Average loss: 0.0184, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 6\n",
    "Train: Loss=0.0050 Batch_id=468 Accuracy=99.18: 100%|██████████| 469/469 [00:19<00:00, 24.55it/s]\n",
    "Test set: Average loss: 0.0181, Accuracy: 9942/10000 (99.42%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 7\n",
    "Train: Loss=0.0155 Batch_id=468 Accuracy=99.17: 100%|██████████| 469/469 [00:18<00:00, 25.54it/s]\n",
    "Test set: Average loss: 0.0177, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 8\n",
    "Train: Loss=0.0030 Batch_id=468 Accuracy=99.19: 100%|██████████| 469/469 [00:18<00:00, 26.00it/s]\n",
    "Test set: Average loss: 0.0174, Accuracy: 9942/10000 (99.42%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 9\n",
    "Train: Loss=0.0018 Batch_id=468 Accuracy=99.22: 100%|██████████| 469/469 [00:17<00:00, 26.61it/s]\n",
    "Test set: Average loss: 0.0170, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 10\n",
    "Train: Loss=0.0150 Batch_id=468 Accuracy=99.25: 100%|██████████| 469/469 [00:18<00:00, 24.88it/s]\n",
    "Test set: Average loss: 0.0175, Accuracy: 9944/10000 (99.44%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 11\n",
    "Train: Loss=0.0513 Batch_id=468 Accuracy=99.26: 100%|██████████| 469/469 [00:17<00:00, 26.45it/s]\n",
    "Test set: Average loss: 0.0169, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 12\n",
    "Train: Loss=0.0195 Batch_id=468 Accuracy=99.26: 100%|██████████| 469/469 [00:18<00:00, 25.21it/s]\n",
    "Test set: Average loss: 0.0166, Accuracy: 9947/10000 (99.47%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 13\n",
    "Train: Loss=0.0043 Batch_id=468 Accuracy=99.28: 100%|██████████| 469/469 [00:19<00:00, 24.49it/s]\n",
    "Test set: Average loss: 0.0168, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 14\n",
    "Train: Loss=0.1060 Batch_id=468 Accuracy=99.28: 100%|██████████| 469/469 [00:17<00:00, 26.29it/s]\n",
    "Test set: Average loss: 0.0170, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 15\n",
    "Train: Loss=0.0171 Batch_id=468 Accuracy=99.24: 100%|██████████| 469/469 [00:18<00:00, 24.91it/s]\n",
    "Test set: Average loss: 0.0168, Accuracy: 9945/10000 (99.45%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.                    [-1, 10, 24, 24]          100\n",
    "|    └─MaxPool2d: 2-10                   [-1, 10, 12, 12]          --\n",
    "├─Sequential: 1-3                        [-1, 16, 4, 4]            --\n",
    "|    └─Conv2d: 2-11                      [-1, 12, 10, 10]          1,080\n",
    "|    └─BatchNorm2d: 2-12                 [-1, 12, 10, 10]          24\n",
    "|    └─ReLU: 2-13                        [-1, 12, 10, 10]          --\n",
    "|    └─Dropout2d: 2-14                   [-1, 12, 10, 10]          --\n",
    "|    └─Conv2d: 2-15                      [-1, 14, 8, 8]            1,512\n",
    "|    └─BatchNorm2d: 2-16                 [-1, 14, 8, 8]            28\n",
    "|    └─ReLU: 2-17                        [-1, 14, 8, 8]            --\n",
    "|    └─Dropout2d: 2-18                   [-1, 14, 8, 8]            --\n",
    "|    └─Conv2d: 2-19                      [-1, 16, 6, 6]            2,016\n",
    "|    └─BatchNorm2d: 2-20                 [-1, 16, 6, 6]            32\n",
    "|    └─ReLU: 2-21                        [-1, 16, 6, 6]            --\n",
    "|    └─Dropout2d: 2-22                   [-1, 16, 6, 6]            --\n",
    "|    └─Conv2d: 2-23                      [-1, 16, 4, 4]            2,304\n",
    "|    └─BatchNorm2d: 2-24                 [-1, 16, 4, 4]            32\n",
    "|    └─ReLU: 2-25                        [-1, 16, 4, 4]            --\n",
    "|    └─Dropout2d: 2-26                   [-1, 16, 4, 4]            --\n",
    "├─Sequential: 1-4                        [-1, 10, 4, 4]            --\n",
    "|    └─Conv2d: 2-27                      [-1, 10, 4, 4]            160\n",
    "├─AdaptiveAvgPool2d: 1-5                 [-1, 10, 1, 1]            --\n",
    "==========================================================================================\n",
    "Total params: 8,116\n",
    "Trainable params: 8,116\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 0.85\n",
    "==========================================================================================\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.26\n",
    "Params size (MB): 0.03\n",
    "Estimated Total Size (MB): 0.29\n",
    "==========================================================================================\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Best Training Accuracy: 99.28**\n",
    "\n",
    "**Best Testing Accuracy : 99.47**\n",
    "\n",
    "\n",
    "`Analysis:`\n",
    "- Still our model overly params\n",
    "- adding few dropout only makes our model realiable. Net8 we did 0.1 dropout rate, here we used 0.01\n",
    "- remove at end of the block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![net8](./pics/net8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Net9_3().to(device)\n",
    "for name,weights in model3.named_parameters():\n",
    "    print(f\"{name}\\t\\t {weights.shape}\")\n",
    "\n",
    "\n",
    "summary(model3,(1,28,28));\n",
    "\n",
    "train_losses = [] ; test_losses = []; train_acc = []; test_acc = []\n",
    "\n",
    "\n",
    "# SAME MODEL BUT DIFFERNT LR\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.3, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1, verbose=True)\n",
    "criterion = F.nll_loss\n",
    "num_epochs = 15\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    train_accuracy, train_loss = train(model3, device, train_loader, optimizer, criterion)\n",
    "    test_accuracy,test_loss    = test(model3, device, test_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "plot_loss_accuracy(train_losses, test_losses, train_acc, test_acc)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```log\n",
    "\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "├─Sequential: 1-1                        [-1, 11, 24, 24]          --\n",
    "|    └─Conv2d: 2-1                       [-1, 8, 26, 26]           72\n",
    "|    └─BatchNorm2d: 2-2                  [-1, 8, 26, 26]           16\n",
    "|    └─ReLU: 2-3                         [-1, 8, 26, 26]           --\n",
    "|    └─Dropout2d: 2-4                    [-1, 8, 26, 26]           --\n",
    "|    └─Conv2d: 2-5                       [-1, 11, 24, 24]          792\n",
    "|    └─BatchNorm2d: 2-6                  [-1, 11, 24, 24]          22\n",
    "|    └─ReLU: 2-7                         [-1, 11, 24, 24]          --\n",
    "|    └─Dropout2d: 2-8                    [-1, 11, 24, 24]          --\n",
    "├─Sequential: 1-2                        [-1, 10, 12, 12]          --\n",
    "|    └─Conv2d: 2-9                       [-1, 10, 24, 24]          110\n",
    "|    └─MaxPool2d: 2-10                   [-1, 10, 12, 12]          --\n",
    "├─Sequential: 1-3                        [-1, 31, 8, 8]            --\n",
    "|    └─Conv2d: 2-11                      [-1, 12, 10, 10]          1,080\n",
    "|    └─BatchNorm2d: 2-12                 [-1, 12, 10, 10]          24\n",
    "|    └─ReLU: 2-13                        [-1, 12, 10, 10]          --\n",
    "|    └─Dropout2d: 2-14                   [-1, 12, 10, 10]          --\n",
    "|    └─Conv2d: 2-15                      [-1, 14, 8, 8]            1,512\n",
    "|    └─BatchNorm2d: 2-16                 [-1, 14, 8, 8]            28\n",
    "|    └─ReLU: 2-17                        [-1, 14, 8, 8]            --\n",
    "|    └─Dropout2d: 2-18                   [-1, 14, 8, 8]            --\n",
    "|    └─Conv2d: 2-19                      [-1, 31, 8, 8]            3,906\n",
    "|    └─BatchNorm2d: 2-20                 [-1, 31, 8, 8]            62\n",
    "|    └─ReLU: 2-21                        [-1, 31, 8, 8]            --\n",
    "|    └─Dropout2d: 2-22                   [-1, 31, 8, 8]            --\n",
    "├─Sequential: 1-4                        [-1, 10, 8, 8]            --\n",
    "|    └─Conv2d: 2-23                      [-1, 10, 8, 8]            310\n",
    "├─AdaptiveAvgPool2d: 1-5                 [-1, 10, 1, 1]            --\n",
    "==========================================================================================\n",
    "Total params: 7,934\n",
    "Trainable params: 7,934\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 1.05\n",
    "==========================================================================================\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.29\n",
    "Params size (MB): 0.03\n",
    "Estimated Total Size (MB): 0.32\n",
    "==========================================================================================\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 1\n",
    "Train: Loss=0.0416 Batch_id=468 Accuracy=90.87: 100%|██████████| 469/469 [00:18<00:00, 25.89it/s]\n",
    "Test set: Average loss: 0.0677, Accuracy: 9794/10000 (97.94%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 2\n",
    "Train: Loss=0.1042 Batch_id=468 Accuracy=97.45: 100%|██████████| 469/469 [00:18<00:00, 25.03it/s]\n",
    "Test set: Average loss: 0.0528, Accuracy: 9844/10000 (98.44%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 3\n",
    "Train: Loss=0.0190 Batch_id=468 Accuracy=98.05: 100%|██████████| 469/469 [00:18<00:00, 24.93it/s]\n",
    "Test set: Average loss: 0.0352, Accuracy: 9879/10000 (98.79%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-01.\n",
    "Epoch 4\n",
    "Train: Loss=0.0570 Batch_id=468 Accuracy=98.39: 100%|██████████| 469/469 [00:17<00:00, 26.42it/s]\n",
    "Test set: Average loss: 0.0409, Accuracy: 9863/10000 (98.63%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 5\n",
    "Train: Loss=0.0286 Batch_id=468 Accuracy=98.84: 100%|██████████| 469/469 [00:18<00:00, 25.20it/s]\n",
    "Test set: Average loss: 0.0239, Accuracy: 9916/10000 (99.16%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 6\n",
    "Train: Loss=0.0120 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:18<00:00, 25.92it/s]\n",
    "Test set: Average loss: 0.0243, Accuracy: 9918/10000 (99.18%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 7\n",
    "Train: Loss=0.0035 Batch_id=468 Accuracy=99.04: 100%|██████████| 469/469 [00:17<00:00, 26.52it/s]\n",
    "Test set: Average loss: 0.0227, Accuracy: 9917/10000 (99.17%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-02.\n",
    "Epoch 8\n",
    "Train: Loss=0.0481 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:18<00:00, 24.83it/s]\n",
    "Test set: Average loss: 0.0234, Accuracy: 9919/10000 (99.19%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 9\n",
    "Train: Loss=0.0070 Batch_id=468 Accuracy=99.13: 100%|██████████| 469/469 [00:17<00:00, 26.18it/s]\n",
    "Test set: Average loss: 0.0229, Accuracy: 9916/10000 (99.16%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 10\n",
    "Train: Loss=0.0717 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:19<00:00, 23.47it/s]\n",
    "Test set: Average loss: 0.0225, Accuracy: 9916/10000 (99.16%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 11\n",
    "Train: Loss=0.0292 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:17<00:00, 26.44it/s]\n",
    "Test set: Average loss: 0.0225, Accuracy: 9918/10000 (99.18%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-03.\n",
    "Epoch 12\n",
    "Train: Loss=0.0415 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:17<00:00, 26.45it/s]\n",
    "Test set: Average loss: 0.0221, Accuracy: 9921/10000 (99.21%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 13\n",
    "Train: Loss=0.0050 Batch_id=468 Accuracy=99.14: 100%|██████████| 469/469 [00:18<00:00, 24.74it/s]\n",
    "Test set: Average loss: 0.0223, Accuracy: 9920/10000 (99.20%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 14\n",
    "Train: Loss=0.0277 Batch_id=468 Accuracy=99.16: 100%|██████████| 469/469 [00:17<00:00, 26.12it/s]\n",
    "Test set: Average loss: 0.0220, Accuracy: 9921/10000 (99.21%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 3.0000e-04.\n",
    "Epoch 15\n",
    "Train: Loss=0.0227 Batch_id=468 Accuracy=99.17: 100%|██████████| 469/469 [00:18<00:00, 25.36it/s]\n",
    "Test set: Average loss: 0.0220, Accuracy: 9919/10000 (99.19%)\n",
    "\n",
    "```\n",
    "\n",
    "Best Training Accuracy: 99.29\n",
    "\n",
    "Best Testing Accuracy : 99.17\n",
    "\n",
    "`Analysis`:\n",
    "- Tweaked to over come number of params:\n",
    "- Accuracy droped .2%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![w9_3_without_aug](./pics/net9_3without_augmentation_adam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Net9_3().to(device)\n",
    "for name,weights in model5.named_parameters():\n",
    "    print(f\"{name}\\t\\t {weights.shape}\")\n",
    "\n",
    "\n",
    "summary(model5,(1,28,28));\n",
    "\n",
    "train_losses = [] ; test_losses = []; train_acc = []; test_acc = []\n",
    "\n",
    "\n",
    "# SAME MODEL BUT DIFFERNT LR\n",
    "optimizer = optim.Adam(model5.parameters(), lr=0.02)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1, verbose=True)\n",
    "criterion = F.nll_loss\n",
    "num_epochs = 15\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    train_accuracy, train_loss = train(model5, device, strain_loader, optimizer, criterion)\n",
    "    test_accuracy,test_loss    = test(model5, device, test_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "plot_loss_accuracy(train_losses, test_losses, train_acc, test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Results:`\n",
    "\n",
    "```log\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "├─Sequential: 1-1                        [-1, 11, 24, 24]          --\n",
    "|    └─Conv2d: 2-1                       [-1, 8, 26, 26]           72\n",
    "|    └─BatchNorm2d: 2-2                  [-1, 8, 26, 26]           16\n",
    "|    └─ReLU: 2-3                         [-1, 8, 26, 26]           --\n",
    "|    └─Dropout2d: 2-4                    [-1, 8, 26, 26]           --\n",
    "|    └─Conv2d: 2-5                       [-1, 11, 24, 24]          792\n",
    "|    └─BatchNorm2d: 2-6                  [-1, 11, 24, 24]          22\n",
    "|    └─ReLU: 2-7                         [-1, 11, 24, 24]          --\n",
    "|    └─Dropout2d: 2-8                    [-1, 11, 24, 24]          --\n",
    "├─Sequential: 1-2                        [-1, 10, 12, 12]          --\n",
    "|    └─Conv2d: 2-9                       [-1, 10, 24, 24]          110\n",
    "|    └─MaxPool2d: 2-10                   [-1, 10, 12, 12]          --\n",
    "├─Sequential: 1-3                        [-1, 31, 8, 8]            --\n",
    "|    └─Conv2d: 2-11                      [-1, 12, 10, 10]          1,080\n",
    "|    └─BatchNorm2d: 2-12                 [-1, 12, 10, 10]          24\n",
    "|    └─ReLU: 2-13                        [-1, 12, 10, 10]          --\n",
    "|    └─Dropout2d: 2-14                   [-1, 12, 10, 10]          --\n",
    "|    └─Conv2d: 2-15                      [-1, 14, 8, 8]            1,512\n",
    "|    └─BatchNorm2d: 2-16                 [-1, 14, 8, 8]            28\n",
    "|    └─ReLU: 2-17                        [-1, 14, 8, 8]            --\n",
    "|    └─Dropout2d: 2-18                   [-1, 14, 8, 8]            --\n",
    "|    └─Conv2d: 2-19                      [-1, 31, 8, 8]            3,906\n",
    "|    └─BatchNorm2d: 2-20                 [-1, 31, 8, 8]            62\n",
    "|    └─ReLU: 2-21                        [-1, 31, 8, 8]            --\n",
    "|    └─Dropout2d: 2-22                   [-1, 31, 8, 8]            --\n",
    "├─Sequential: 1-4                        [-1, 10, 8, 8]            --\n",
    "|    └─Conv2d: 2-23                      [-1, 10, 8, 8]            310\n",
    "├─AdaptiveAvgPool2d: 1-5                 [-1, 10, 1, 1]            --\n",
    "==========================================================================================\n",
    "Total params: 7,934\n",
    "Trainable params: 7,934\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 1.05\n",
    "==========================================================================================\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.29\n",
    "Params size (MB): 0.03\n",
    "Estimated Total Size (MB): 0.32\n",
    "==========================================================================================\n",
    "==========================================================================================\n",
    "Adjusting learning rate of group 0 to 2.0000e-02.\n",
    "Epoch 1\n",
    "Train: Loss=0.2057 Batch_id=468 Accuracy=88.57: 100%|██████████| 469/469 [00:29<00:00, 15.69it/s]\n",
    "Test set: Average loss: 0.0738, Accuracy: 9760/10000 (97.60%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-02.\n",
    "Epoch 2\n",
    "Train: Loss=0.1501 Batch_id=468 Accuracy=96.13: 100%|██████████| 469/469 [00:30<00:00, 15.56it/s]\n",
    "Test set: Average loss: 0.0474, Accuracy: 9841/10000 (98.41%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-02.\n",
    "Epoch 3\n",
    "Train: Loss=0.0235 Batch_id=468 Accuracy=97.01: 100%|██████████| 469/469 [00:29<00:00, 15.90it/s]\n",
    "Test set: Average loss: 0.0775, Accuracy: 9796/10000 (97.96%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-02.\n",
    "Epoch 4\n",
    "Train: Loss=0.1349 Batch_id=468 Accuracy=97.17: 100%|██████████| 469/469 [00:28<00:00, 16.33it/s]\n",
    "Test set: Average loss: 0.0354, Accuracy: 9884/10000 (98.84%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-03.\n",
    "Epoch 5\n",
    "Train: Loss=0.0801 Batch_id=468 Accuracy=98.12: 100%|██████████| 469/469 [00:28<00:00, 16.18it/s]\n",
    "Test set: Average loss: 0.0206, Accuracy: 9931/10000 (99.31%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-03.\n",
    "Epoch 6\n",
    "Train: Loss=0.0234 Batch_id=468 Accuracy=98.34: 100%|██████████| 469/469 [00:28<00:00, 16.31it/s]\n",
    "Test set: Average loss: 0.0196, Accuracy: 9936/10000 (99.36%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-03.\n",
    "Epoch 7\n",
    "Train: Loss=0.0366 Batch_id=468 Accuracy=98.35: 100%|██████████| 469/469 [00:29<00:00, 15.79it/s]\n",
    "Test set: Average loss: 0.0202, Accuracy: 9930/10000 (99.30%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-03.\n",
    "Epoch 8\n",
    "Train: Loss=0.0406 Batch_id=468 Accuracy=98.40: 100%|██████████| 469/469 [00:28<00:00, 16.23it/s]\n",
    "Test set: Average loss: 0.0184, Accuracy: 9941/10000 (99.41%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-04.\n",
    "Epoch 9\n",
    "Train: Loss=0.0170 Batch_id=468 Accuracy=98.41: 100%|██████████| 469/469 [00:28<00:00, 16.41it/s]\n",
    "Test set: Average loss: 0.0179, Accuracy: 9937/10000 (99.37%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-04.\n",
    "Epoch 10\n",
    "Train: Loss=0.0139 Batch_id=468 Accuracy=98.53: 100%|██████████| 469/469 [00:29<00:00, 15.87it/s]\n",
    "Test set: Average loss: 0.0178, Accuracy: 9940/10000 (99.40%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-04.\n",
    "Epoch 11\n",
    "Train: Loss=0.0786 Batch_id=468 Accuracy=98.39: 100%|██████████| 469/469 [00:28<00:00, 16.35it/s]\n",
    "Test set: Average loss: 0.0178, Accuracy: 9941/10000 (99.41%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-04.\n",
    "Epoch 12\n",
    "Train: Loss=0.1173 Batch_id=468 Accuracy=98.47: 100%|██████████| 469/469 [00:30<00:00, 15.55it/s]\n",
    "Test set: Average loss: 0.0177, Accuracy: 9939/10000 (99.39%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-05.\n",
    "Epoch 13\n",
    "Train: Loss=0.0365 Batch_id=468 Accuracy=98.57: 100%|██████████| 469/469 [00:28<00:00, 16.45it/s]\n",
    "Test set: Average loss: 0.0178, Accuracy: 9938/10000 (99.38%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-05.\n",
    "Epoch 14\n",
    "Train: Loss=0.0105 Batch_id=468 Accuracy=98.47: 100%|██████████| 469/469 [00:28<00:00, 16.56it/s]\n",
    "Test set: Average loss: 0.0178, Accuracy: 9943/10000 (99.43%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-05.\n",
    "Epoch 15\n",
    "Train: Loss=0.0304 Batch_id=468 Accuracy=98.51: 100%|██████████| 469/469 [00:28<00:00, 16.19it/s]\n",
    "Test set: Average loss: 0.0173, Accuracy: 9939/10000 (99.39%)\n",
    "\n",
    "Adjusting learning rate of group 0 to 2.0000e-05.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Best Training Accuracy: 98.53**\n",
    "\n",
    "**Best Testing Accuracy : 99.43**\n",
    "\n",
    "`Analysis`:\n",
    "- Augmentatio helps to somewhat overcome previous accuracy dropdowns\n",
    "- Adam helps me to reach 99.43 validation accuracy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final_model](./pics/w9_aug_final_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e025b110c800109cd58230379fbbfe8e65738d242bfa90b1d92b8d7d0ac4d7ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
