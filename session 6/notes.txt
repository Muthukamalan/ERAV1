
How many layers we should add,
mp
1*1
3*3

receptive field
kernels and how do we decide the number of kernels?
batch normalization

position of maxpooling

concepts of transition layer
position of Transition layer

dropout, when we know we've some overfitting
distance of Mp from prediction
distance of batch norm from prediction


when do we stop conv and go ahead with a large kernel or some other alternative ( which we not covered yet ) **GAP**

How do we know our network is not going well, compertively very early


batch size and effects of batch size











take code ---> 99.45% validation/test accuracy

less than 20K params and 20 epochs

use
- Batch norm
- FC                     **not forced**
- have used GAP          **good to hv**               **extra point**





**Solutions**

- How many layers i've 20000 params think like that
- MP    In Mnist there's no texture,pattern,parts of object and object //// only texture and pattern maximum 1 or 2 MP
- 1*1 compress total number of channels
- 3*3 increase channels
- RF must be same of image I've 
- softmax last layer
- lr=0.001
- kernels and how do we decide number of kernels  **GAP** FC makes larger
- batch norm add every layer 
- Position of MaxPooling 
- concept of Transition Layers 1*1 and MP together
- positionof Transition Layers =position of MP

- Dropout like salt to every layer
- when we add GAP? first time I and overfitting with train and test 

- when do we stop conv and go ahead with larger kernel or some other alternative
        **big kernel alternative**--------------> **GAP**

- how do we know our n/w is not going well,

